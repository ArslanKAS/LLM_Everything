{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19f55cad",
   "metadata": {},
   "source": [
    "# LLM Evaluation\n",
    "\n",
    "* **ROGUE (Recall-Oriented Understudy for Gisting Evaluation):**\n",
    "    * **ROUGE-1:** Counts how many individual words are shared between the machine-generated summary and the original summary.\n",
    "    * **ROUGE-2:** Counts how many pairs of words (like \"important information\") are shared between the two summaries.\n",
    "    * **ROUGE-L:** Looks for the longest sequence of words that appears in both summaries, like a sentence or phrase.\n",
    "    * **ROUGE-Lsum:** Adds up the longest sequences from all the original summaries to get a total score.\n",
    "---\n",
    "* **BLEU (Bilingual Evaluation Understudy):**\n",
    "    * **BLEU score:** A score (between 0 and 1) that measures how well the GenAI text matches the original text.\n",
    "    * **Precisions:** Measures how many words (or phrases) are shared between the two texts:\n",
    "        * 1-gram: Counts individual words\n",
    "        * 2-gram: Counts pairs of words\n",
    "        * 3-gram: Counts trios of words\n",
    "        * 4-gram: Counts quads of words\n",
    "    * **Brevity penalty:** A penalty applied if the GenAI text is too short compared to the original text (1.0 means no penalty).\n",
    "    * **Length ratio:** Compares the length of the GenAI text to the original text (e.g., 1.16 means the GenAI text is 16% longer).\n",
    "    * **Translation length and Reference length:** The number of words in the GenAI text and the original text, respectively.\n",
    "---\n",
    "* **METEOR (Metric for Evaluation for Translation with Explicit Ordering):**\n",
    "    * METEOR is a metric used for GenAI evaluation and is said to have a better correlation with human judgment.\n",
    "    * It modifies the precision and recall computations, replacing them with a weighted F-score based on mapping unigrams and a penalty function for incorrect word order.\n",
    "    * It aims to solve the problem of other metrics not accounting for word order in the candidate text.\n",
    "---\n",
    "* **BERTScore**:\n",
    "    * BERTScore is a metric used to evaluate the quality of text summarization by measuring how similar the text summary is to the original text.\n",
    "    * It uses contextualized token embeddings, which are shown to be effective for entailment detection.\n",
    "    * It leverages the power of BERT, a state-of-the-art transformer-based model developed by Google, to understand the semantic meaning of words in a sentence.\n",
    "    * It provides **precision, recall and F1-scores**, allowing users to have a comprehensive understanding of the performance of their models.\n",
    "---\n",
    "* **Perplexity**:\n",
    "    * Perplexity is a metric used to evaluate the performance of language models (LLMs). In simple terms, perplexity measures how well the model predicts a text sample.\n",
    "        * **Low perplexity (close to 0):** The LLM is very confident and accurate in its predictions, like it's \"not perplexed\" at all!\n",
    "        * **High perplexity (close to 1 or higher):** The LLM is uncertain or \"perplexed\" about the text, indicating poor predictions.\n",
    "    * **Perplexities:** This is a list of perplexity scores for each input text. A lower perplexity score indicates better prediction performance.\n",
    "    * **Mean Perplexity:** This is the average perplexity score across all input texts. It provides a single value representing the overall perplexity.\n",
    "---\n",
    "* **MoverScore**:\n",
    "    * This scorer first uses embedding models, specifically pre-trained language models like BERT to obtain deeply contextualized word embeddings for both the reference text and the generated text \n",
    "    * Afterwards it uses something called the Earth Moverâ€™s Distance (EMD) to compute the minimal cost that must be paid to transform the distribution of words in an LLM output to the distribution of words in the reference text.\n",
    "---\n",
    "* **BLEURT**:\n",
    "    * It does the same thing as BERTScore but is better correlated with human judgment compared to similar metrics such as BERT and BERTscore\n",
    "    * The scores are generally between 0 and 1, with higher values indicating greater similarity between the texts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d6cf95",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade -r \"/home/ec2-user/SageMaker/15. Essential Code/requirements.txt\" -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bf961a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install evaluate -q\n",
    "!pip install rouge-score -q\n",
    "!pip install bert-score -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a39cce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cea3da96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1a8285b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lvwerra/test',\n",
       " 'jordyvl/ece',\n",
       " 'angelina-wang/directional_bias_amplification',\n",
       " 'cpllab/syntaxgym',\n",
       " 'lvwerra/bary_score',\n",
       " 'hack/test_metric',\n",
       " 'yzha/ctc_eval',\n",
       " 'codeparrot/apps_metric',\n",
       " 'mfumanelli/geometric_mean',\n",
       " 'daiyizheng/valid',\n",
       " 'erntkn/dice_coefficient',\n",
       " 'mgfrantz/roc_auc_macro',\n",
       " 'Vlasta/pr_auc',\n",
       " 'gorkaartola/metric_for_tp_fp_samples',\n",
       " 'idsedykh/metric',\n",
       " 'idsedykh/codebleu2',\n",
       " 'idsedykh/codebleu',\n",
       " 'idsedykh/megaglue',\n",
       " 'cakiki/ndcg',\n",
       " 'Vertaix/vendiscore',\n",
       " 'GMFTBY/dailydialogevaluate',\n",
       " 'GMFTBY/dailydialog_evaluate',\n",
       " 'jzm-mailchimp/joshs_second_test_metric',\n",
       " 'ola13/precision_at_k',\n",
       " 'yulong-me/yl_metric',\n",
       " 'abidlabs/mean_iou',\n",
       " 'abidlabs/mean_iou2',\n",
       " 'KevinSpaghetti/accuracyk',\n",
       " 'NimaBoscarino/weat',\n",
       " 'ronaldahmed/nwentfaithfulness',\n",
       " 'Viona/infolm',\n",
       " 'kyokote/my_metric2',\n",
       " 'kashif/mape',\n",
       " 'Ochiroo/rouge_mn',\n",
       " 'giulio98/code_eval_outputs',\n",
       " 'leslyarun/fbeta_score',\n",
       " 'giulio98/codebleu',\n",
       " 'anz2/iliauniiccocrevaluation',\n",
       " 'zbeloki/m2',\n",
       " 'xu1998hz/sescore',\n",
       " 'dvitel/codebleu',\n",
       " 'NCSOFT/harim_plus',\n",
       " 'JP-SystemsX/nDCG',\n",
       " 'sportlosos/sescore',\n",
       " 'Drunper/metrica_tesi',\n",
       " 'jpxkqx/peak_signal_to_noise_ratio',\n",
       " 'jpxkqx/signal_to_reconstruction_error',\n",
       " 'hpi-dhc/FairEval',\n",
       " 'lvwerra/accuracy_score',\n",
       " 'ybelkada/cocoevaluate',\n",
       " 'harshhpareek/bertscore',\n",
       " 'posicube/mean_reciprocal_rank',\n",
       " 'bstrai/classification_report',\n",
       " 'omidf/squad_precision_recall',\n",
       " 'Josh98/nl2bash_m',\n",
       " 'BucketHeadP65/confusion_matrix',\n",
       " 'BucketHeadP65/roc_curve',\n",
       " 'yonting/average_precision_score',\n",
       " 'transZ/test_parascore',\n",
       " 'transZ/sbert_cosine',\n",
       " 'hynky/sklearn_proxy',\n",
       " 'xu1998hz/sescore_english_mt',\n",
       " 'xu1998hz/sescore_german_mt',\n",
       " 'xu1998hz/sescore_english_coco',\n",
       " 'xu1998hz/sescore_english_webnlg',\n",
       " 'unnati/kendall_tau_distance',\n",
       " 'Viona/fuzzy_reordering',\n",
       " 'Viona/kendall_tau',\n",
       " 'lhy/hamming_loss',\n",
       " 'lhy/ranking_loss',\n",
       " 'Muennighoff/code_eval_octopack',\n",
       " 'yuyijiong/quad_match_score',\n",
       " 'Splend1dchan/cosine_similarity',\n",
       " 'AlhitawiMohammed22/CER_Hu-Evaluation-Metrics',\n",
       " 'Yeshwant123/mcc',\n",
       " 'transformersegmentation/segmentation_scores',\n",
       " 'sma2023/wil',\n",
       " 'chanelcolgate/average_precision',\n",
       " 'ckb/unigram',\n",
       " 'Felipehonorato/eer',\n",
       " 'manueldeprada/beer',\n",
       " 'shunzh/apps_metric',\n",
       " 'He-Xingwei/sari_metric',\n",
       " 'langdonholmes/cohen_weighted_kappa',\n",
       " 'fschlatt/ner_eval',\n",
       " 'hyperml/balanced_accuracy',\n",
       " 'brian920128/doc_retrieve_metrics',\n",
       " 'guydav/restrictedpython_code_eval',\n",
       " 'k4black/codebleu',\n",
       " 'Natooz/ece',\n",
       " 'ingyu/klue_mrc',\n",
       " 'Vipitis/shadermatch',\n",
       " 'unitxt/metric',\n",
       " 'gabeorlanski/bc_eval',\n",
       " 'jjkim0807/code_eval',\n",
       " 'vichyt/metric-codebleu',\n",
       " 'repllabs/mean_reciprocal_rank',\n",
       " 'repllabs/mean_average_precision',\n",
       " 'mtc/fragments',\n",
       " 'DarrenChensformer/eval_keyphrase',\n",
       " 'kedudzic/charmatch',\n",
       " 'Vallp/ter',\n",
       " 'DarrenChensformer/relation_extraction',\n",
       " 'Ikala-allen/relation_extraction',\n",
       " 'danieldux/hierarchical_softmax_loss',\n",
       " 'nlpln/tst',\n",
       " 'bdsaglam/jer',\n",
       " 'davebulaval/meaningbert',\n",
       " 'fnvls/bleu1234',\n",
       " 'fnvls/bleu_1234',\n",
       " 'nevikw39/specificity',\n",
       " 'yqsong/execution_accuracy',\n",
       " 'shalakasatheesh/squad_v2',\n",
       " 'arthurvqin/pr_auc',\n",
       " 'd-matrix/dmx_perplexity',\n",
       " 'akki2825/accents_unplugged_eval',\n",
       " 'juliakaczor/accents_unplugged_eval',\n",
       " 'chimene/accents_unplugged_eval',\n",
       " 'Vickyage/accents_unplugged_eval',\n",
       " 'Qui-nn/accents_unplugged_eval',\n",
       " 'TelEl/accents_unplugged_eval',\n",
       " 'livvie/accents_unplugged_eval',\n",
       " 'DaliaCaRo/accents_unplugged_eval',\n",
       " 'alvinasvk/accents_unplugged_eval',\n",
       " 'LottieW/accents_unplugged_eval',\n",
       " 'LuckiestOne/valid_efficiency_score',\n",
       " 'Fritz02/execution_accuracy',\n",
       " 'huanghuayu/multiclass_brier_score',\n",
       " 'jialinsong/apps_metric',\n",
       " 'DoctorSlimm/bangalore_score',\n",
       " 'agkphysics/ccc',\n",
       " 'DoctorSlimm/kaushiks_criteria',\n",
       " 'CZLC/rouge_raw',\n",
       " 'bascobasculino/mot-metrics',\n",
       " 'SEA-AI/mot-metrics',\n",
       " 'SEA-AI/det-metrics',\n",
       " 'saicharan2804/my_metric',\n",
       " 'red1bluelost/evaluate_genericify_cpp',\n",
       " 'maksymdolgikh/seqeval_with_fbeta',\n",
       " 'Bekhouche/NED',\n",
       " 'danieldux/isco_hierarchical_accuracy',\n",
       " 'ginic/phone_errors',\n",
       " 'haotongye-shopee/ppl',\n",
       " 'berkatil/map',\n",
       " 'DarrenChensformer/action_generation',\n",
       " 'buelfhood/fbeta_score',\n",
       " 'danasone/ru_errant',\n",
       " 'helena-balabin/youden_index',\n",
       " 'SEA-AI/PanopticQuality',\n",
       " 'ncoop57/levenshtein_distance',\n",
       " 'kaleidophon/almost_stochastic_order',\n",
       " 'lvwerra/element_count',\n",
       " 'prb977/cooccurrence_count',\n",
       " 'NimaBoscarino/pseudo_perplexity',\n",
       " 'ybelkada/toxicity',\n",
       " 'ronaldahmed/ccl_win',\n",
       " 'cakiki/tokens_per_byte',\n",
       " 'lsy641/distinct',\n",
       " 'grepLeigh/perplexity',\n",
       " 'Charles95/element_count',\n",
       " 'Charles95/accuracy']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of Evaluation Modules\n",
    "\n",
    "evaluate.list_evaluation_modules()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2185b240",
   "metadata": {},
   "source": [
    "## Summary to be Tested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d3a4ffa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_summary = \"\"\"\n",
    "Meta Platforms, Inc., formerly Facebook, Inc., is a major American technology conglomerate based in Menlo Park, California.\n",
    "It owns and operates Facebook, Instagram, Threads, and WhatsApp. Meta ranks among the largest tech companies globally, \n",
    "alongside Alphabet (Google), Amazon, Apple, and Microsoft. Despite ventures into hardware, Meta relies heavily on \n",
    "advertising for revenue, comprising 97.8% in 2023. It rebranded to Meta Platforms, Inc. in 2021 to emphasize its \n",
    "focus on building the metaverse, integrating its products and services. Through acquisitions and partnerships, \n",
    "Meta continues to expand its reach and influence in the tech industry.\n",
    "\"\"\"\n",
    "\n",
    "llm_summary = \"\"\"\n",
    "Meta Platforms, Inc., formerly known as Facebook, Inc., is a prominent American technology conglomerate headquartered in \n",
    "Menlo Park, California. The company's portfolio includes popular platforms such as Facebook, Instagram, \n",
    "Threads, and WhatsApp. It is considered one of the leading technology companies globally, competing with giants \n",
    "like Alphabet (Google), Amazon, Apple, and Microsoft. Despite dabbling in hardware ventures, Meta predominantly \n",
    "relies on advertising, which accounted for 97.8% of its revenue in 2023. In 2021, the company rebranded as \n",
    "Meta Platforms, Inc. to underscore its commitment to building the metaverse, an integrated environment connecting \n",
    "its products and services. Through acquisitions and partnerships, Meta continues to expand its influence and presence \n",
    "in the tech sector.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d011a591",
   "metadata": {},
   "source": [
    "## ROGUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80b87fa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99ad40f3fc944fc7bbfc3dcba4438019",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rogue_scorer = evaluate.load('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9fe023ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': 0.7019230769230769,\n",
       " 'rouge2': 0.4271844660194175,\n",
       " 'rougeL': 0.6346153846153846,\n",
       " 'rougeLsum': 0.6538461538461539}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rogue_scorer.compute(references=[actual_summary], predictions=[llm_summary])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a31bd57",
   "metadata": {},
   "source": [
    "## BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03e004df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4234c2c81be742ca8b684715f11d8720",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/5.94k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b5ccdfcc7374b8fb272f4dfb3d7ca24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/1.55k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c90e4cd609c14c0794ae097ce217f841",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/3.34k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bleu_scorer = evaluate.load('bleu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c76a935",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bleu': 0.45040483379141744,\n",
       " 'precisions': [0.7112676056338029,\n",
       "  0.49645390070921985,\n",
       "  0.38571428571428573,\n",
       "  0.302158273381295],\n",
       " 'brevity_penalty': 1.0,\n",
       " 'length_ratio': 1.1639344262295082,\n",
       " 'translation_length': 142,\n",
       " 'reference_length': 122}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu_scorer.compute(references=[actual_summary], predictions=[llm_summary])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792b8055",
   "metadata": {},
   "source": [
    "## METEOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "339c3721",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa1358619ea84bc5bd93a55a068d9834",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.93k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/ec2-user/nltk_data...\n",
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package omw-1.4 to /home/ec2-user/nltk_data...\n"
     ]
    }
   ],
   "source": [
    "meteor_scorer = evaluate.load('meteor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ace9721",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'meteor': 0.7225239471511148}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meteor_scorer.compute(references=[actual_summary], predictions=[llm_summary])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e233efc",
   "metadata": {},
   "source": [
    "## BertScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e2b4925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.8955]), tensor([0.8448]), tensor([0.8694]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Method 1\n",
    "from bert_score import BERTScorer\n",
    "bert_scorer = BERTScorer(model_type='bert-base-uncased')\n",
    "bert_scorer.score([actual_summary], [llm_summary])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "316a86ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'precision': [0.9383306503295898],\n",
       " 'recall': [0.9495557546615601],\n",
       " 'f1': [0.9439098834991455],\n",
       " 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.40.0)'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Method 2\n",
    "bert_scorer = evaluate.load(\"bertscore\")\n",
    "bert_scorer.compute(references=[actual_summary], predictions=[llm_summary], lang=\"en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7feaa61e",
   "metadata": {},
   "source": [
    "## Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e0b6bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexity_scorer = evaluate.load(\"perplexity\", module_type=\"metric\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "205d8963",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba3d405c82504582b3762f249bdf4d44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'perplexities': [44.8903923034668], 'mean_perplexity': 44.8903923034668}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity_scorer.compute(references=[actual_summary], predictions=[llm_summary], model_id = \"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01048fd",
   "metadata": {},
   "source": [
    "## BLEURT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5199ab40",
   "metadata": {},
   "outputs": [],
   "source": [
    "bleurt_scorer = evaluate.load(\"bleurt\", module_type=\"metric\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354e6713",
   "metadata": {},
   "outputs": [],
   "source": [
    "bleurt_scorer.compute(references=[actual_summary], predictions=[llm_summary])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
